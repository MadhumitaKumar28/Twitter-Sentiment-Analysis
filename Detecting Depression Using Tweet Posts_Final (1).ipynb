{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e19ab86",
   "metadata": {},
   "source": [
    "# Detecting Depression on Social Media Using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b3814e",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb55be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install twint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86eabb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "318d8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b8fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install twint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b934e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82a6661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5684610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4cf63b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install twint --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "127ca632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pip==21.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4827a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+git://github.com/ajctrl/twint@patch-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3098b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import ftfy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from math import exp\n",
    "from numpy import sign \n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a515861",
   "metadata": {},
   "source": [
    "## Data Collection Using Twint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b68d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --user --upgrade -e git+https://github.com/twintproject/twint.git@origin/master#egg=twint;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc736bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint\n",
    "import datetime\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845c268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = twint.Config()\n",
    "c.Tor = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da69601",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_country =['USA', 'Afghanistan', 'Aland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia, Plurinational State of', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo, The Democratic Republic of the', 'Cook Islands', 'Costa Rica', \"Côte d'Ivoire\", 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See (Vatican City State)', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran, Islamic Republic of', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', \"Korea, Democratic People's Republic of\", 'Korea, Republic of', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia, Republic of', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia, Federated States of', 'Moldova, Republic of', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territory, Occupied', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Réunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'South Sudan', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan, Province of China', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'United States of America', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela, Bolivarian Republic of', 'Viet Nam', 'Virgin Islands, British', 'Virgin Islands, U.S.', 'Wallis and Futuna', 'Yemen', 'Zambia', 'Zimbabwe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b113328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afghanistan\n"
     ]
    },
    {
     "ename": "RefreshTokenException",
     "evalue": "Could not find the Guest token in HTML",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRefreshTokenException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m         c\u001b[38;5;241m.\u001b[39mLimit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     19\u001b[0m         twint\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mSearch(c)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mscrape_by_country\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdepressed_tweets.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mscrape_by_country\u001b[0;34m(outfile)\u001b[0m\n\u001b[1;32m     17\u001b[0m c\u001b[38;5;241m.\u001b[39mResume \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresume.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     18\u001b[0m c\u001b[38;5;241m.\u001b[39mLimit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtwint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/twint/run.py:410\u001b[0m, in \u001b[0;36mSearch\u001b[0;34m(config, callback)\u001b[0m\n\u001b[1;32m    408\u001b[0m config\u001b[38;5;241m.\u001b[39mFollowers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    409\u001b[0m config\u001b[38;5;241m.\u001b[39mProfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mPandas_au:\n\u001b[1;32m    412\u001b[0m     storage\u001b[38;5;241m.\u001b[39mpanda\u001b[38;5;241m.\u001b[39m_autoget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/twint/run.py:329\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(config, callback)\u001b[0m\n\u001b[1;32m    325\u001b[0m     logme\u001b[38;5;241m.\u001b[39mexception(\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:run:Unexpected exception occurred while attempting to get or create a new event loop.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m get_event_loop()\u001b[38;5;241m.\u001b[39mrun_until_complete(\u001b[43mTwint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmain(callback))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/twint/run.py:36\u001b[0m, in \u001b[0;36mTwint.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# TODO might have to make some adjustments for it to work with multi-treading\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# USAGE : to get a new guest token simply do `self.token.refresh()`\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mToken(config)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mConn(config\u001b[38;5;241m.\u001b[39mDatabase)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md \u001b[38;5;241m=\u001b[39m datelock\u001b[38;5;241m.\u001b[39mSet(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mUntil, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mSince)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/twint/token.py:69\u001b[0m, in \u001b[0;36mToken.refresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mGuest_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RefreshTokenException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find the Guest token in HTML\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRefreshTokenException\u001b[0m: Could not find the Guest token in HTML"
     ]
    }
   ],
   "source": [
    "def scrape_by_country(outfile):\n",
    "    unique_country=set(all_country) #To get unique cities of country\n",
    "    countries = sorted(unique_country) #Sort & convert datatype to list\n",
    "    for country in countries:\n",
    "        print(country)\n",
    "        c = twint.Config()\n",
    "        c.Search = 'anxiety' #search keyword\n",
    "        c.Since = str(datetime.datetime(2018, 3, 1))\n",
    "        c.Until = str(datetime.datetime(2022, 3, 1))\n",
    "        c.Store_csv = True\n",
    "        c.Output = \"./\" + outfile\n",
    "        c.Near = country\n",
    "        c.Hide_output = True\n",
    "        c.Count = True\n",
    "        c.Stats = True\n",
    "        c.Lang = 'en'\n",
    "        c.Resume = 'resume.txt'\n",
    "        c.Limit = 1000\n",
    "        twint.run.Search(c)\n",
    "\n",
    "scrape_by_country('depressed_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef3aa4",
   "metadata": {},
   "source": [
    "## Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d13fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "depressive_tweets_df = pd.read_csv('depressed_tweets.csv')\n",
    "random_tweets_df = pd.read_csv('random_tweets.csv')\n",
    "\n",
    "# doubt what is this file? \n",
    "EMBEDDING_FILE = r'C:\\Users\\katle\\PycharmProjects\\AIMT Term 2\\AML 2304\\Detecting Depression using Tweets\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07ecb50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13649, 37)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressive_tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf39b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20215, 37)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d10f97",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a20281",
   "metadata": {},
   "source": [
    "### Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1ce527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants #Reproducibility #Try later\n",
    "\n",
    "np.random.seed(1234)\n",
    "DEPRES_NROWS = 12000 # number of rows to read from depressive_tweets\n",
    "RANDOM_NROWS = 12000 # number of rows to read from random_tweets\n",
    "MAX_SEQUENCE_LENGTH = 140 # max tweet size\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "TRAIN_SPLIT = 0.6\n",
    "TEST_SPLIT = 0.2\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da0cdd47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unable to handle scheme 'c', expected one of ('', 'file', 'hdfs', 'http', 'https', 'scp', 'sftp', 'ssh', 'webhdfs'). Extra dependencies required by 'c' may be missing. See <https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst> for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m word2vec \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEMBEDDING_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m     \u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gensim/models/keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2045\u001b[0m             counts[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(count)\n\u001b[1;32m   2047\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading projection weights from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, fname)\n\u001b[0;32m-> 2048\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_header:\n\u001b[1;32m   2050\u001b[0m         \u001b[38;5;66;03m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/smart_open/smart_open_lib.py:235\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(ve\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 235\u001b[0m binary \u001b[38;5;241m=\u001b[39m \u001b[43m_open_binary_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransport_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m decompressed \u001b[38;5;241m=\u001b[39m so_compression\u001b[38;5;241m.\u001b[39mcompression_wrapper(binary, binary_mode, compression)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mor\u001b[39;00m explicit_encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/smart_open/smart_open_lib.py:397\u001b[0m, in \u001b[0;36m_open_binary_stream\u001b[0;34m(uri, mode, transport_params)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know how to handle uri \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mrepr\u001b[39m(uri))\n\u001b[1;32m    396\u001b[0m scheme \u001b[38;5;241m=\u001b[39m _sniff_scheme(uri)\n\u001b[0;32m--> 397\u001b[0m submodule \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_transport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheme\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m fobj \u001b[38;5;241m=\u001b[39m submodule\u001b[38;5;241m.\u001b[39mopen_uri(uri, mode, transport_params)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/smart_open/transport.py:93\u001b[0m, in \u001b[0;36mget_transport\u001b[0;34m(scheme)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheme \u001b[38;5;129;01min\u001b[39;00m _REGISTRY:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _REGISTRY[scheme]\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(message)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Unable to handle scheme 'c', expected one of ('', 'file', 'hdfs', 'http', 'https', 'scp', 'sftp', 'ssh', 'webhdfs'). Extra dependencies required by 'c' may be missing. See <https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst> for details."
     ]
    }
   ],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True) #doubt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76b1650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand Contraction\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "626c2f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    cleaned_tweets = []\n",
    "    for tweet in tweets:\n",
    "        tweet = str(tweet)\n",
    "        #if re.match('(\\w+:\\/\\/\\S+)', tweet) == None and len(tweet) > 10:\n",
    "        tweet = ' '.join(re.sub('(@[A-Za-z0-9]+)|(\\#[A-Za-z0-9]+)|(<Emoji:.*>)|(pic\\.twitter\\.com\\/.*)', ' ', tweet).split())\n",
    "\n",
    "        tweet = ftfy.fix_text(tweet)\n",
    "            \n",
    "        tweet = expandContractions(tweet)\n",
    "            \n",
    "        tweet = ' '.join(re.sub('([^0-9A-Za-z \\t])', ' ', tweet).split())\n",
    "        tweet = ' '.join(re.sub('http\\S+', ' ', tweet).split())\n",
    "            \n",
    "        myOwn_stopwords = ['depress']\n",
    "        stop_words = set(stopwords.words('english') + myOwn_stopwords)\n",
    "        word_tokens = nltk.word_tokenize(tweet)\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "        tweet = ' '.join(filtered_sentence)\n",
    "            \n",
    "        tweet = PorterStemmer().stem(tweet)\n",
    "            \n",
    "        cleaned_tweets.append(tweet)\n",
    "            \n",
    "    return cleaned_tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a94ba11",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1600e113",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/Users/madhumitakumar/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/share/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sid \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentIntensityAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m depressive_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiments\u001b[39m\u001b[38;5;124m'\u001b[39m]           \u001b[38;5;241m=\u001b[39m depressive_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: sid\u001b[38;5;241m.\u001b[39mpolarity_scores(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m'\u001b[39m,x\u001b[38;5;241m.\u001b[39mlower()))))\n\u001b[1;32m      3\u001b[0m depressive_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]   \u001b[38;5;241m=\u001b[39m depressive_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiments\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m)) \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/sentiment/vader.py:340\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[0;34m(self, lexicon_file)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     lexicon_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m ):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon_file \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexicon_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_lex_dict()\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants \u001b[38;5;241m=\u001b[39m VaderConstants()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/Users/madhumitakumar/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/share/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "depressive_tweets_df['sentiments']           = depressive_tweets_df['tweet'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\n",
    "depressive_tweets_df['positive sentiment']   = depressive_tweets_df['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \n",
    "depressive_tweets_df['neutral sentiment']    = depressive_tweets_df['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\n",
    "depressive_tweets_df['negative sentiment']   = depressive_tweets_df['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n",
    "depressive_tweets_df['compound']  = depressive_tweets_df['sentiments'].apply(lambda score_dict: score_dict['compound'])\n",
    "depressive_tweets_df['comp_score'] = depressive_tweets_df['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3adf2e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiments\u001b[39m\u001b[38;5;124m'\u001b[39m]           \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_tweets_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtweet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolarity_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mw+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]   \u001b[38;5;241m=\u001b[39m random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiments\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m)) \n\u001b[1;32m      3\u001b[0m random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]    \u001b[38;5;241m=\u001b[39m random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiments\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneu\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiments\u001b[39m\u001b[38;5;124m'\u001b[39m]           \u001b[38;5;241m=\u001b[39m random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43msid\u001b[49m\u001b[38;5;241m.\u001b[39mpolarity_scores(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m'\u001b[39m,x\u001b[38;5;241m.\u001b[39mlower()))))\n\u001b[1;32m      2\u001b[0m random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]   \u001b[38;5;241m=\u001b[39m random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiments\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m)) \n\u001b[1;32m      3\u001b[0m random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]    \u001b[38;5;241m=\u001b[39m random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiments\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneu\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sid' is not defined"
     ]
    }
   ],
   "source": [
    "random_tweets_df['sentiments']           = random_tweets_df['tweet'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\n",
    "random_tweets_df['positive sentiment']   = random_tweets_df['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \n",
    "random_tweets_df['neutral sentiment']    = random_tweets_df['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\n",
    "random_tweets_df['negative sentiment']   = random_tweets_df['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n",
    "random_tweets_df['compound']  = random_tweets_df['sentiments'].apply(lambda score_dict: score_dict['compound'])\n",
    "random_tweets_df['comp_score'] = random_tweets_df['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf732f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eaa1023",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'negative sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'negative sentiment'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComparison of Negative Sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m,fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m19\u001b[39m,fontweight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbold\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m ax0 \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mkdeplot(\u001b[43mdepressive_tweets_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnegative sentiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,bw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      7\u001b[0m kde_x, kde_y \u001b[38;5;241m=\u001b[39m ax0\u001b[38;5;241m.\u001b[39mlines[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_data()\n\u001b[1;32m      8\u001b[0m ax0\u001b[38;5;241m.\u001b[39mfill_between(kde_x, kde_y, where\u001b[38;5;241m=\u001b[39m(kde_x\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.25\u001b[39m) , \n\u001b[1;32m      9\u001b[0m                 interpolate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtab:blue\u001b[39m\u001b[38;5;124m'\u001b[39m,alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'negative sentiment'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1700x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={\"figure.figsize\":(17, 10)})\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('Comparison of Negative Sentiment',fontsize=19,fontweight='bold')\n",
    "\n",
    "ax0 = sns.kdeplot(depressive_tweets_df['negative sentiment'],bw=0.1)\n",
    "\n",
    "kde_x, kde_y = ax0.lines[0].get_data()\n",
    "ax0.fill_between(kde_x, kde_y, where=(kde_x>0.25) , \n",
    "                interpolate=True, color='tab:blue',alpha=0.6)\n",
    "\n",
    "plt.annotate('Depressive Tweets', xy=(0.25, 0.5), xytext=(0.4, 2),\n",
    "            arrowprops=dict(facecolor='red', shrink=0.05),fontsize=16,fontweight='bold')\n",
    "\n",
    "ax0.axvline(depressive_tweets_df['negative sentiment'].mean(), color='r', linestyle='--')\n",
    "ax0.axvline(depressive_tweets_df['negative sentiment'].median(), color='tab:orange', linestyle='-')\n",
    "plt.legend({'PDF':depressive_tweets_df['negative sentiment'],r'Mean: {:.2f}'.format(depressive_tweets_df['negative sentiment'].mean()):depressive_tweets_df['negative sentiment'].mean(),\n",
    "            r'Median: {:.2f}'.format(depressive_tweets_df['negative sentiment'].median()):depressive_tweets_df['negative sentiment'].median()})\n",
    "\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "ax1 = sns.kdeplot(random_tweets_df['negative sentiment'],bw=0.1,color='green')\n",
    "\n",
    "plt.annotate('Random Tweets', xy=(0.25, 0.5), xytext=(0.4, 2),\n",
    "            arrowprops=dict(facecolor='red', shrink=0.05),fontsize=16,fontweight='bold')\n",
    "kde_x, kde_y = ax1.lines[0].get_data()\n",
    "ax1.fill_between(kde_x, kde_y, where=(kde_x>0.25) , \n",
    "                interpolate=True, color='tab:green',alpha=0.6)\n",
    "ax1.set_xlabel('Sentiment Strength',fontsize=18)\n",
    "\n",
    "\n",
    "ax1.axvline(random_tweets_df['negative sentiment'].mean(), color='r', linestyle='--')\n",
    "ax1.axvline(random_tweets_df['negative sentiment'].median(), color='tab:orange', linestyle='-')\n",
    "plt.legend({'PDF':random_tweets_df['negative sentiment'],r'Mean: {:.2f}'.format(random_tweets_df['negative sentiment'].mean()):random_tweets_df['negative sentiment'].mean(),\n",
    "            r'Median: {:.2f}'.format(random_tweets_df['negative sentiment'].median()):random_tweets_df['negative sentiment'].median()})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b434f7",
   "metadata": {},
   "source": [
    "### Comparison of Common Words Between Depressive and Random Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a7ede8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/madhumitakumar/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/share/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/madhumitakumar/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/share/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#doubt what is done here?\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m depressive_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mclean_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepressive_tweets_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtweet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m clean_tweets(random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m, in \u001b[0;36mclean_tweets\u001b[0;34m(tweets)\u001b[0m\n\u001b[1;32m     13\u001b[0m tweet \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, tweet)\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m     15\u001b[0m myOwn_stopwords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepress\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 16\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m myOwn_stopwords)\n\u001b[1;32m     17\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(tweet)\n\u001b[1;32m     18\u001b[0m filtered_sentence \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/madhumitakumar/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/share/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#doubt what is done here?\n",
    "depressive_tweets_df['clean'] = clean_tweets(depressive_tweets_df['tweet'])\n",
    "random_tweets_df['clean'] = clean_tweets(random_tweets_df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7aefedb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'positive sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'positive sentiment'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m depressive_tweets_pos \u001b[38;5;241m=\u001b[39m depressive_tweets_df[\u001b[43mdepressive_tweets_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpositive sentiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mbetween(\u001b[38;5;241m0.40\u001b[39m,\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      2\u001b[0m random_tweets_pos \u001b[38;5;241m=\u001b[39m random_tweets_df[random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbetween(\u001b[38;5;241m0.40\u001b[39m,\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'positive sentiment'"
     ]
    }
   ],
   "source": [
    "depressive_tweets_pos = depressive_tweets_df[depressive_tweets_df['positive sentiment'].between(0.40,1)]\n",
    "random_tweets_pos = random_tweets_df[random_tweets_df['positive sentiment'].between(0.40,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e041856e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'depressive_tweets_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m depressive_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mdepressive_tweets_pos\u001b[49m\u001b[38;5;241m.\u001b[39mclean)\n\u001b[1;32m      2\u001b[0m random_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(random_tweets_pos\u001b[38;5;241m.\u001b[39mclean)\n\u001b[1;32m      5\u001b[0m pwc \u001b[38;5;241m=\u001b[39m WordCloud(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m,height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,collocations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mgenerate(depressive_pos)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'depressive_tweets_pos' is not defined"
     ]
    }
   ],
   "source": [
    "depressive_pos = ' '.join(depressive_tweets_pos.clean)\n",
    "random_pos = ' '.join(random_tweets_pos.clean)\n",
    "\n",
    "\n",
    "pwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(depressive_pos)\n",
    "nwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(random_pos)\n",
    "\n",
    "sns.set(rc={\"figure.figsize\":(17, 10)})\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Common Words Among Depressive Tweets',fontsize=16,fontweight='bold')\n",
    "plt.imshow(pwc)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Common Words Among RandomTweets',fontsize=16,fontweight='bold')\n",
    "plt.imshow(nwc)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "484b9e69",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'negative sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'negative sentiment'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m depressive_tweets_neg \u001b[38;5;241m=\u001b[39m depressive_tweets_df[\u001b[43mdepressive_tweets_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnegative sentiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mbetween(\u001b[38;5;241m0.25\u001b[39m,\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      2\u001b[0m random_tweets_neg \u001b[38;5;241m=\u001b[39m random_tweets_df[random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbetween(\u001b[38;5;241m0.25\u001b[39m,\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'negative sentiment'"
     ]
    }
   ],
   "source": [
    "depressive_tweets_neg = depressive_tweets_df[depressive_tweets_df['negative sentiment'].between(0.25,1)]\n",
    "random_tweets_neg = random_tweets_df[random_tweets_df['negative sentiment'].between(0.25,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3de55282",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'depressive_tweets_neg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m depressive_neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mdepressive_tweets_neg\u001b[49m\u001b[38;5;241m.\u001b[39mclean)\n\u001b[1;32m      2\u001b[0m random_neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(random_tweets_neg\u001b[38;5;241m.\u001b[39mclean)\n\u001b[1;32m      5\u001b[0m pwc \u001b[38;5;241m=\u001b[39m WordCloud(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m,height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,collocations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mgenerate(depressive_neg)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'depressive_tweets_neg' is not defined"
     ]
    }
   ],
   "source": [
    "depressive_neg = ' '.join(depressive_tweets_neg.clean)\n",
    "random_neg = ' '.join(random_tweets_neg.clean)\n",
    "\n",
    "\n",
    "pwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(depressive_neg)\n",
    "nwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(random_neg)\n",
    "\n",
    "sns.set(rc={\"figure.figsize\":(17, 10)})\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Common Words Among Depressive Tweets',fontsize=16,fontweight='bold')\n",
    "plt.imshow(pwc)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Common Words Among RandomTweets',fontsize=16,fontweight='bold')\n",
    "plt.imshow(nwc)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93501c3d",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11acc207",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "581d62ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/madhumitakumar/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/share/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/madhumitakumar/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/share/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m depressive_tweets_arr \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m depressive_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      2\u001b[0m random_tweets_arr \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m random_tweets_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m----> 3\u001b[0m X_d \u001b[38;5;241m=\u001b[39m \u001b[43mclean_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepressive_tweets_arr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m X_r \u001b[38;5;241m=\u001b[39m clean_tweets(random_tweets_arr)\n",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m, in \u001b[0;36mclean_tweets\u001b[0;34m(tweets)\u001b[0m\n\u001b[1;32m     13\u001b[0m tweet \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, tweet)\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m     15\u001b[0m myOwn_stopwords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepress\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 16\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m myOwn_stopwords)\n\u001b[1;32m     17\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(tweet)\n\u001b[1;32m     18\u001b[0m filtered_sentence \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/madhumitakumar/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/share/nltk_data'\n    - '/Users/madhumitakumar/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "depressive_tweets_arr = [x for x in depressive_tweets_df['tweet']]\n",
    "random_tweets_arr = [x for x in random_tweets_df['tweet']]\n",
    "X_d = clean_tweets(depressive_tweets_arr)\n",
    "X_r = clean_tweets(random_tweets_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69b6179d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(num_words\u001b[38;5;241m=\u001b[39mMAX_NB_WORDS)\n\u001b[0;32m----> 2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(\u001b[43mX_d\u001b[49m \u001b[38;5;241m+\u001b[39m X_r)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_d' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X_d + X_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16db9c83",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sequences_d \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(\u001b[43mX_d\u001b[49m)\n\u001b[1;32m      2\u001b[0m sequences_r \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_r)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_d' is not defined"
     ]
    }
   ],
   "source": [
    "sequences_d = tokenizer.texts_to_sequences(X_d)\n",
    "sequences_r = tokenizer.texts_to_sequences(X_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c5c5a42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX_d\u001b[49m[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_d' is not defined"
     ]
    }
   ],
   "source": [
    "X_d[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d612832e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequences_d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msequences_d\u001b[49m[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sequences_d' is not defined"
     ]
    }
   ],
   "source": [
    "sequences_d[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f198643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 unique tokens\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7111af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ec843ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequences_d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_d \u001b[38;5;241m=\u001b[39m pad_sequences(\u001b[43msequences_d\u001b[49m[:\u001b[38;5;241m12000\u001b[39m], maxlen\u001b[38;5;241m=\u001b[39mMAX_SEQUENCE_LENGTH)\n\u001b[1;32m      2\u001b[0m data_r \u001b[38;5;241m=\u001b[39m pad_sequences(sequences_r[:\u001b[38;5;241m12000\u001b[39m], maxlen\u001b[38;5;241m=\u001b[39mMAX_SEQUENCE_LENGTH)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of data_d tensor:\u001b[39m\u001b[38;5;124m'\u001b[39m, data_d\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sequences_d' is not defined"
     ]
    }
   ],
   "source": [
    "data_d = pad_sequences(sequences_d[:12000], maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_r = pad_sequences(sequences_r[:12000], maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data_d tensor:', data_d.shape)\n",
    "print('Shape of data_r tensor:', data_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c89397f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for (word, idx) in word_index.items():\n",
    "    if word in word2vec.key_to_index and idx < MAX_NB_WORDS:\n",
    "        embedding_matrix[idx] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56a08bf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m labels_r \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m RANDOM_NROWS)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Splitting the arrays into test (60%), validation (20%), and train data (20%)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m perm_d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(\u001b[38;5;28mlen\u001b[39m(\u001b[43mdata_d\u001b[49m))\n\u001b[1;32m      7\u001b[0m idx_train_d \u001b[38;5;241m=\u001b[39m perm_d[:\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_d)\u001b[38;5;241m*\u001b[39m(TRAIN_SPLIT))]\n\u001b[1;32m      8\u001b[0m idx_test_d \u001b[38;5;241m=\u001b[39m perm_d[\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_d)\u001b[38;5;241m*\u001b[39m(TRAIN_SPLIT)):\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_d)\u001b[38;5;241m*\u001b[39m(TRAIN_SPLIT\u001b[38;5;241m+\u001b[39mTEST_SPLIT))]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_d' is not defined"
     ]
    }
   ],
   "source": [
    "# Assigning labels to the depressive tweets and random tweets data\n",
    "labels_d = np.array([1] * DEPRES_NROWS)\n",
    "labels_r = np.array([0] * RANDOM_NROWS)\n",
    "\n",
    "# Splitting the arrays into test (60%), validation (20%), and train data (20%)\n",
    "perm_d = np.random.permutation(len(data_d))\n",
    "idx_train_d = perm_d[:int(len(data_d)*(TRAIN_SPLIT))]\n",
    "idx_test_d = perm_d[int(len(data_d)*(TRAIN_SPLIT)):int(len(data_d)*(TRAIN_SPLIT+TEST_SPLIT))]\n",
    "idx_val_d = perm_d[int(len(data_d)*(TRAIN_SPLIT+TEST_SPLIT)):]\n",
    "\n",
    "perm_r = np.random.permutation(len(data_r))\n",
    "idx_train_r = perm_r[:int(len(data_r)*(TRAIN_SPLIT))]\n",
    "idx_test_r = perm_r[int(len(data_r)*(TRAIN_SPLIT)):int(len(data_r)*(TRAIN_SPLIT+TEST_SPLIT))]\n",
    "idx_val_r = perm_r[int(len(data_r)*(TRAIN_SPLIT+TEST_SPLIT)):]\n",
    "\n",
    "# Combine depressive tweets and random tweets arrays\n",
    "data_train = np.concatenate((data_d[idx_train_d], data_r[idx_train_r]))\n",
    "labels_train = np.concatenate((labels_d[idx_train_d], labels_r[idx_train_r]))\n",
    "data_test = np.concatenate((data_d[idx_test_d], data_r[idx_test_r]))\n",
    "labels_test = np.concatenate((labels_d[idx_test_d], labels_r[idx_test_r]))\n",
    "data_val = np.concatenate((data_d[idx_val_d], data_r[idx_val_r]))\n",
    "labels_val = np.concatenate((labels_d[idx_val_d], labels_r[idx_val_r]))\n",
    "\n",
    "# Shuffling\n",
    "perm_train = np.random.permutation(len(data_train))\n",
    "data_train = data_train[perm_train]\n",
    "labels_train = labels_train[perm_train]\n",
    "perm_test = np.random.permutation(len(data_test))\n",
    "data_test = data_test[perm_test]\n",
    "labels_test = labels_test[perm_test]\n",
    "perm_val = np.random.permutation(len(data_val))\n",
    "data_val = data_val[perm_val]\n",
    "labels_val = labels_val[perm_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d61407",
   "metadata": {},
   "source": [
    "### Creating a Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbac0f87",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (23432020.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[31], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"`\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class LogReg:\n",
    "    \"\"\"\n",
    "    Class to represent a logistic regression model.\n",
    "    \"\"\"`\n",
    "\n",
    "    def __init__(self, l_rate, epochs, n_features):\n",
    "        \"\"\"\n",
    "        Create a new model with certain parameters.\n",
    "\n",
    "        :param l_rate: Initial learning rate for model.\n",
    "        :param epoch: Number of epochs to train for.\n",
    "        :param n_features: Number of features.\n",
    "        \"\"\"\n",
    "        self.l_rate = l_rate\n",
    "        self.epochs = epochs\n",
    "        self.coef = [0.0] * n_features\n",
    "        self.bias = 0.0\n",
    "\n",
    "    def sigmoid(self, score, threshold=20.0):\n",
    "        \"\"\"\n",
    "        Prevent overflow of exp by capping activation at 20.\n",
    "\n",
    "        :param score: A real valued number to convert into a number between 0 and 1\n",
    "        \"\"\"\n",
    "        if abs(score) > threshold:\n",
    "            score = threshold * sign(score)\n",
    "        activation = exp(score)\n",
    "        return activation / (1.0 + activation)\n",
    "\n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Given an example's features and the coefficients, predicts the class.\n",
    "\n",
    "        :param features: List of real valued features for a single training example.\n",
    "\n",
    "        :return: Returns the predicted class (either 0 or 1).\n",
    "        \"\"\"\n",
    "        value = sum([features[i]*self.coef[i] for i in range(len(features))]) + self.bias\n",
    "        return self.sigmoid(value)\n",
    "\n",
    "    def sg_update(self, features, label):\n",
    "        \"\"\"\n",
    "        Computes the update to the weights based on a predicted example.\n",
    "\n",
    "        :param features: Features to train on.\n",
    "        :param label: Corresponding label for features.\n",
    "        \"\"\"\n",
    "        yhat = self.predict(features)\n",
    "        e = label - yhat\n",
    "        self.bias = self.bias + self.l_rate * e * yhat * (1-yhat)\n",
    "        for i in range(len(features)):\n",
    "            self.coef[i] = self.coef[i] + self.l_rate * e * yhat * (1-yhat) * features[i]\n",
    "        return\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes logistic regression coefficients using stochastic gradient descent.\n",
    "\n",
    "        :param X: Features to train on.\n",
    "        :param y: Corresponding label for each set of features.\n",
    "\n",
    "        :return: Returns a list of model weight coefficients where coef[0] is the bias.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            for features, label in zip(X, y):\n",
    "                self.sg_update(features, label)\n",
    "        return self.bias, self.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3363d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_bar, y_pred):\n",
    "    \"\"\"\n",
    "    Computes what percent of the total testing data the model classified correctly.\n",
    "\n",
    "    :param y_bar: List of ground truth classes for each example.\n",
    "    :param y_pred: List of model predicted class for each example.\n",
    "\n",
    "    :return: Returns a real number between 0 and 1 for the model accuracy.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for i in range(len(y_bar)):\n",
    "        if y_bar[i] == y_pred[i]:\n",
    "            correct += 1\n",
    "    accuracy = (correct / len(y_bar)) * 100.0\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e8e6288",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogReg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Logistic Model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m logreg \u001b[38;5;241m=\u001b[39m \u001b[43mLogReg\u001b[49m(LEARNING_RATE, EPOCHS, \u001b[38;5;28mlen\u001b[39m(data_train[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      3\u001b[0m bias_logreg, weights_logreg \u001b[38;5;241m=\u001b[39m logreg\u001b[38;5;241m.\u001b[39mtrain(data_train, labels_train)\n\u001b[1;32m      4\u001b[0m y_logistic \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mround\u001b[39m(logreg\u001b[38;5;241m.\u001b[39mpredict(example)) \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m data_test]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogReg' is not defined"
     ]
    }
   ],
   "source": [
    "# Logistic Model\n",
    "logreg = LogReg(LEARNING_RATE, EPOCHS, len(data_train[0]))\n",
    "bias_logreg, weights_logreg = logreg.train(data_train, labels_train)\n",
    "y_logistic = [round(logreg.predict(example)) for example in data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28a7b488",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_logistic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compare accuracies\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m accuracy_logistic \u001b[38;5;241m=\u001b[39m get_accuracy(\u001b[43my_logistic\u001b[49m, labels_test)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression Accuracy: \u001b[39m\u001b[38;5;132;01m{:0.3f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(accuracy_logistic))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_logistic' is not defined"
     ]
    }
   ],
   "source": [
    "# Compare accuracies\n",
    "accuracy_logistic = get_accuracy(y_logistic, labels_test)\n",
    "print('Logistic Regression Accuracy: {:0.3f}'.format(accuracy_logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbbc9676",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(\u001b[43mlabels_test\u001b[49m, y_logistic))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_test, y_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada5aba",
   "metadata": {},
   "source": [
    "### Building Model LSTM + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Embedded layer\n",
    "model.add(Embedding(len(embedding_matrix), EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "# Convolutional Layer\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "# LSTM Layer\n",
    "model.add(LSTM(300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf1828",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "hist = model.fit(data_train, labels_train, \\\n",
    "        validation_data=(data_val, labels_val), \\\n",
    "        epochs=EPOCHS, batch_size=40, shuffle=True, \\\n",
    "        callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ab9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e4b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f32bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = model.predict(data_test)\n",
    "labels_pred = np.round(labels_pred.flatten())\n",
    "accuracy = accuracy_score(labels_test, labels_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02236f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_test, labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8449b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
